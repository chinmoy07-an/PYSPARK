{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LETS START WITH FIRST PYSPARK SESSION**\n",
    "\n",
    "*Note- make sure anaconda or miniconda is pre-installed in your system*\n",
    "\n",
    "First using the below command check which version of python the Jupyter notebook is using.\n",
    "\n",
    "**IMPORTANT**\n",
    "*If python version is shown as 3.8 then there is a possibility that pyspark import may not work\n",
    "due to mismatch of python and spark versions.So you need to install or downgrade your python version*\n",
    "\n",
    "**To do this follow the below steps**\n",
    "1. Run *conda create -n py37 python=3.7 anaconda* in cmd (version is optional you can go for any version less than 3.8)\n",
    "\n",
    "2. Once the above completes run *conda info --envs*\n",
    "*This will currently point to latest environments*\n",
    "\n",
    "3. Now run *conda activate py37*\n",
    "\n",
    "4. Check *conda info --envs* again.It should be pointing now to the downgraded envs.\n",
    "\n",
    "5. Now close existing jupyter notebook sessions and run *jupyter notebook* again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.9\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the environment variables set just to be sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Spark\\spark-2.4.7-bin-hadoop2.7\n",
      "C:\\Program Files\\Java\\jdk1.8.0_221\n",
      "C:\\app\\Dell\\product\\18.0.0\\dbhomeXE\\bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Java\\jdk-12.0.2\\bin;C:\\Program Files\\Java\\jdk-12.0.2\\bin;C:\\Program Files\\PuTTY\\;C:\\Program Files\\Git\\cmd;C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python37-32;C:\\Users\\Dell\\miniconda3;C:\\Users\\Dell\\miniconda3\\Library\\bin;C:\\Users\\Dell\\miniconda3\\Scripts;C:\\Program Files\\Java\\jdk1.8.0_221\\bin;C:\\Users\\Dell\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\Dell\\miniconda3\\Library\\bin;C:\\Users\\Dell\\miniconda3;C:\\Users\\Dell\\miniconda3\\Scripts;C:\\Users\\Dell\\Spark\\spark-2.4.7-bin-hadoop2.7\\bin;\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRE-SETUPs to check**\n",
    "\n",
    "*You can check in cmd*\n",
    "\n",
    "1.Java --version should be 1.8\n",
    "\n",
    "2.Check if environment variables are set for JAVA_HOME\n",
    "\n",
    "3.Download Spark and extract the file in chosen directory i.e C:\\Spark\n",
    "\n",
    "[spark](http://spark.apache.org/downloads.html)\n",
    "\n",
    "4.Setup the environment variables\n",
    "*SPARK_HOME  = C:\\spark\\spark-2.3.2-bin-hadoop2.7\n",
    "HADOOP_HOME = C:\\spark\\spark-2.3.2-bin-hadoop2.7*\n",
    "\n",
    "5.Add following path to PATH variable\n",
    "\n",
    "*C:\\spark\\spark-2.3.2-bin-hadoop2.7\\bin*\n",
    "\n",
    "6.Download and save winutils.exe in SPARK_HOME/bin directory\n",
    "\n",
    "*You can use below link to download winutils.exe but make sure to\n",
    "download the exe file from correct Hadoop version folder*\n",
    "\n",
    "[winutils](https://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exe)\n",
    "\n",
    "7.create folder C:\\tmp\\hive\n",
    "\n",
    "8.Execute the following commands in cmd\n",
    "\n",
    "*winutils.exe chmod -R 777 C:\\tmp\\hive\n",
    "\n",
    "winutils.exe ls -F C:\\tmp\\hive*\n",
    "\n",
    "9.Once done check pyspark is running or not in cmd by typing *pyspark*\n",
    "\n",
    "10.To come out of spark shell type exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the above command**\n",
    "\n",
    "*If it shows error module not found*\n",
    "Run the below command in cmd.\n",
    "\n",
    "*conda install -c conda-forge findspark*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the below command to output SPARK_HOME path . Just a Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Dell\\\\Spark\\\\spark-2.4.7-bin-hadoop2.7'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the pyspark by setting up spark context and conf and running a simple test example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4])\n",
    "nums.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VOILA!!! THE SETUP IS DONE**\n",
    "\n",
    "IF you get any issues please reach out to me on linkedin.\n",
    "\n",
    "[LinkedIn](https://www.linkedin.com/in/chinmoy-anand-b56a4812b)\n",
    "\n",
    "*THANKS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
